{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d76ed61e-2d15-40a6-953d-45d905c6eccf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import itertools\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "310b34ee-b2d7-4a89-8fa3-4e15f83a30d9",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Create helper functions\n",
    "\n",
    "adapted from https://github.com/spdrnl/ecg/blob/master/ECG.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2e5f0504-40a4-4edf-9afa-3017187a3e7b",
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Pe_yCVYMtnq9",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Copied from https://github.com/avanwyk/tensorflow-projects/blob/master/lr-finder/lr_finder.py\n",
    "# Apache License 2.0\n",
    "\n",
    "from tensorflow.keras.callbacks import Callback\n",
    "\n",
    "class LRFinder(Callback):\n",
    "    \"\"\"`Callback` that exponentially adjusts the learning rate after each training batch between `start_lr` and\n",
    "    `end_lr` for a maximum number of batches: `max_step`. The loss and learning rate are recorded at each step allowing\n",
    "    visually finding a good learning rate as per https://sgugger.github.io/how-do-you-find-a-good-learning-rate.html via\n",
    "    the `plot` method.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, start_lr: float = 1e-7, end_lr: float = 10, max_steps: int = 100, smoothing=0.9):\n",
    "        super(LRFinder, self).__init__()\n",
    "        self.start_lr, self.end_lr = start_lr, end_lr\n",
    "        self.max_steps = max_steps\n",
    "        self.smoothing = smoothing\n",
    "        self.step, self.best_loss, self.avg_loss, self.lr = 0, 0, 0, 0\n",
    "        self.lrs, self.losses = [], []\n",
    "\n",
    "    def on_train_begin(self, logs=None):\n",
    "        self.step, self.best_loss, self.avg_loss, self.lr = 0, 0, 0, 0\n",
    "        self.lrs, self.losses = [], []\n",
    "\n",
    "    def on_train_batch_begin(self, batch, logs=None):\n",
    "        self.lr = self.exp_annealing(self.step)\n",
    "        tf.keras.backend.set_value(self.model.optimizer.lr, self.lr)\n",
    "\n",
    "    def on_train_batch_end(self, batch, logs=None):\n",
    "        logs = logs or {}\n",
    "        loss = logs.get('loss')\n",
    "        step = self.step\n",
    "        if loss:\n",
    "            self.avg_loss = self.smoothing * self.avg_loss + (1 - self.smoothing) * loss\n",
    "            smooth_loss = self.avg_loss / (1 - self.smoothing ** (self.step + 1))\n",
    "            self.losses.append(smooth_loss)\n",
    "            self.lrs.append(self.lr)\n",
    "\n",
    "            if step == 0 or loss < self.best_loss:\n",
    "                self.best_loss = loss\n",
    "\n",
    "            if smooth_loss > 4 * self.best_loss or tf.math.is_nan(smooth_loss):\n",
    "                self.model.stop_training = True\n",
    "\n",
    "        if step == self.max_steps:\n",
    "            self.model.stop_training = True\n",
    "\n",
    "        self.step += 1\n",
    "\n",
    "    def exp_annealing(self, step):\n",
    "        return self.start_lr * (self.end_lr / self.start_lr) ** (step * 1. / self.max_steps)\n",
    "\n",
    "    def plot(self):\n",
    "        fig, ax = plt.subplots(1, 1)\n",
    "        ax.set_ylabel('Loss')\n",
    "        ax.set_xlabel('Learning Rate (log scale)')\n",
    "        ax.set_xscale('log')\n",
    "        ax.xaxis.set_major_formatter(plt.FormatStrFormatter('%.0e'))\n",
    "        ax.plot(self.lrs, self.losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b407ad2f-c579-4c7f-9281-3d9572e1584b",
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "QaCy6KbsWGme",
    "tags": []
   },
   "outputs": [],
   "source": [
    "def pretty_plot(history, field, fn):\n",
    "    def plot(data, val_data, best_index, best_value, title):\n",
    "        plt.plot(range(1, len(data)+1), data, label='train')\n",
    "        plt.plot(range(1, len(data)+1), val_data, label='validation')\n",
    "        if not best_index is None:\n",
    "            plt.axvline(x=best_index+1, linestyle=':', c=\"#777777\")\n",
    "        if not best_value is None:\n",
    "            plt.axhline(y=best_value, linestyle=':', c=\"#777777\")\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.ylabel(field)\n",
    "        plt.xticks(range(0, len(data), 20))\n",
    "        plt.title(title)\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "\n",
    "    data = history.history[field]\n",
    "    val_data = history.history['val_' + field]\n",
    "    tail = int(0.15 * len(data))\n",
    "\n",
    "    best_index = fn(val_data)\n",
    "    best_value = val_data[best_index]\n",
    "\n",
    "    plot(data, val_data, best_index, best_value, \"{} over epochs (best {:06.4f})\".format(field, best_value))\n",
    "    plot(data[-tail:], val_data[-tail:], None, best_value, \"{} over last {} epochs\".format(field, tail))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0c700bb0-ca8a-4c08-8bc0-caa3f6130010",
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xvAzK8OgpnQl",
    "tags": []
   },
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(cm, classes,\n",
    "                          normalize=False,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=plt.cm.Blues):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        print(\"Normalized confusion matrix\")\n",
    "    else:\n",
    "        print('Confusion matrix, without normalization')\n",
    "\n",
    "    print(cm)\n",
    "\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, cm[i, j],\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f262603-a3ae-4e0c-986a-9dec4efafd0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fix_shape_conv(x_lst, y_lst):\n",
    "    X_train, X_test, X_val = x_lst\n",
    "    y_train, y_test, y_val = y_lst\n",
    "    \n",
    "    if len(X_train.shape) == 2:\n",
    "        X_train = np.expand_dims(X_train, -1)\n",
    "        X_val = np.expand_dims(X_val, -1)\n",
    "        X_test = np.expand_dims(X_test, -1)\n",
    "\n",
    "    if len(y_train.shape) == 1:\n",
    "        y_val = np.expand_dims(y_val, -1)\n",
    "        y_train = np.expand_dims(y_train, -1)\n",
    "        y_test = np.expand_dims(y_test, -1)\n",
    "    \n",
    "    return (X_train, X_test, X_val), (y_train, y_test, y_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba82198b-c750-4737-a54b-126872441f85",
   "metadata": {},
   "source": [
    "## Timeseries normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5f40bda3-1dec-40b1-8807-f886e822fa99",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_data(X_train, X_test, X_val, method=4):\n",
    "    if method == 1:\n",
    "        X_train -= X_train.min()\n",
    "        X_train /= X_train.max()\n",
    "        X_test -= X_test.min()\n",
    "        X_test /= X_test.max()\n",
    "        X_val -= X_val.min()\n",
    "        X_val /= X_val.max()\n",
    "    elif method == 2:\n",
    "        train_mean = X_train.mean()\n",
    "        train_std = X_train.std()\n",
    "        X_train = ((X_train.T - train_mean) / train_std).T\n",
    "        X_test = ((X_test.T - train_mean) / train_std).T\n",
    "        X_val = ((X_val.T - train_mean) / train_std).T\n",
    "    elif method == 3:\n",
    "        X_train = ((X_train.T - X_train.mean(1)) / X_train.std(1)).T\n",
    "        X_test = ((X_test.T - X_test.mean(1)) / X_test.std(1)).T\n",
    "        X_val = ((X_val.T - X_val.mean(1)) / X_val.std(1)).T\n",
    "    elif method == 4:\n",
    "        # cf. https://github.com/helme/ecg_ptbxl_benchmarking/blob/master/code/utils/utils.py\n",
    "        from sklearn.preprocessing import StandardScaler\n",
    "        # Standardize data such that mean 0 and variance 1\n",
    "        ss = StandardScaler()\n",
    "        ss.fit(np.vstack(X_train).flatten()[:,np.newaxis].astype(float))\n",
    "\n",
    "        def apply_standardizer(X, ss):\n",
    "            X_tmp = []\n",
    "            for x in X:\n",
    "                x_shape = x.shape\n",
    "                X_tmp.append(ss.transform(x.flatten()[:,np.newaxis]).reshape(x_shape))\n",
    "            X_tmp = np.array(X_tmp)\n",
    "            return X_tmp\n",
    "\n",
    "        # X_train, X_val, X_test = map(apply_standardizer, (X_train, X_val, X_test))\n",
    "        X_train = apply_standardizer(X_train, ss)\n",
    "        X_test = apply_standardizer(X_test, ss)\n",
    "        X_val = apply_standardizer(X_val, ss)\n",
    "\n",
    "    return X_train, X_test, X_val"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
